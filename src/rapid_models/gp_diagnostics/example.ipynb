{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "### __Multifold cross-validation for GP regression__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from nptyping import Complex, Float, Int, NDArray, Shape\n",
    "\n",
    "# Import GP cross validation and metrics library\n",
    "import rapid_models.gp_diagnostics.cv as gp_cv\n",
    "import rapid_models.gp_diagnostics.metrics as gp_metrics\n",
    "# For GP modelling\n",
    "import rapid_models.gp_models.utils as gputils\n",
    "from rapid_models.gp_diagnostics.utils.stats import snorm_qq\n",
    "from rapid_models.gp_models.templates import ExactGPModel\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# DNV primary colors \n",
    "clr = {\n",
    "    'Sky blue':'#99d9f0',\n",
    "    'Land green':'#3f9c35',\n",
    "    'Sea blue':'#003591',\n",
    "    'Dark blue':'#0f204b',\n",
    "    'Cyan':'#009fda',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate some dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42) \n",
    "\n",
    "N_DIM: int = 2 # the X dimension\n",
    "N_TRAIN: int = 12\n",
    "\n",
    "KER_SCALE_TRUE: float = 1.0\n",
    "KER_LENGTHSCALE_TRUE: torch.Tensor = torch.ones(N_DIM+1)*0.5\n",
    "KER_LENGTHSCALE_TRUE[-1] = KER_LENGTHSCALE_TRUE[-1]*0.5 # Shorter lengthscale in the t-dimension\n",
    "\n",
    "N_T_TRAIN: NDArray[Shape['N_TRAIN'], Int] \n",
    "N_T_TRAIN = np.random.randint(5, 20, size = N_TRAIN) # Different number of observations in the t-dimension for each observed function \n",
    "\n",
    "T_TRAIN: List[NDArray[Shape['*'], Float]]\n",
    "T_TRAIN = [np.linspace(np.random.uniform(0, 0.1), np.random.uniform(0.9, 1.0), n) for n in N_T_TRAIN]\n",
    "\n",
    "X_TRAIN: NDArray[Shape['N_TRAIN, N_DIM'], Float] \n",
    "X_TRAIN = np.array([\n",
    "    [0.82, 0.88], \n",
    "    [0.84, 0.81],\n",
    "    [0.7, 0.22], \n",
    "    [0.2, 0.6]\n",
    "])\n",
    "X_TRAIN = np.append(X_TRAIN, np.random.uniform(size = (N_TRAIN-4, N_DIM)), axis = 0)\n",
    "\n",
    "from_to: List[int] = [0] + list(N_T_TRAIN.cumsum())\n",
    "FOLDS_INDICES: List[List[int]] = [list(range(from_to[i], from_to[i+1])) for i in range(N_TRAIN)]\n",
    "\n",
    "N_XT_TRAIN: int = int(N_T_TRAIN.sum())\n",
    "N_XT_DIM: int = N_DIM + 1\n",
    "\n",
    "XT_TRAIN: NDArray[Shape['N_XT_TRAIN, N_XT_DIM'], Float]\n",
    "XT_TRAIN = np.zeros((N_XT_TRAIN, N_XT_DIM))\n",
    "for i in range(N_TRAIN):\n",
    "    XT_TRAIN[FOLDS_INDICES[i], 0:N_DIM] = X_TRAIN[i]\n",
    "    XT_TRAIN[FOLDS_INDICES[i], -1] = T_TRAIN[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define kernel and sample training data\n",
    "\n",
    "# Kernel\n",
    "ker: gpytorch.kernels.Kernel = gputils.gpytorch_kernel_Matern(\n",
    "    outputscale=KER_SCALE_TRUE, \n",
    "    lengthscale=KER_LENGTHSCALE_TRUE,\n",
    ")\n",
    "\n",
    "# Covariance matrix; shape=(N_XT_TRAIN, N_XT_TRAIN)\n",
    "K: gpytorch.lazy.LazyEvaluatedKernelTensor = ker(torch.tensor(XT_TRAIN, dtype=torch.float))  # type: ignore\n",
    "\n",
    "# Distribution\n",
    "normal_rv: gpytorch.distributions.Distribution = gpytorch.distributions.MultivariateNormal(\n",
    "    mean = torch.zeros(K.shape[0]),  # vector; shape=(N_XT_TRAIN)\n",
    "    covariance_matrix = K,  # matrix; shape=(N_XT_TRAIN, N_XT_TRAIN)\n",
    ")\n",
    "\n",
    "# Sample training data\n",
    "Y_TRAIN: torch.Tensor = normal_rv.sample()  # vector; shape=(N_XT_TRAIN)\n",
    "YT_TRAIN: List[torch.Tensor] = [Y_TRAIN[t_idx] for t_idx in FOLDS_INDICES]\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols = 2, figsize = (20, 8), gridspec_kw={'width_ratios': [1, 2]})\n",
    "axs = axs.ravel() \n",
    "\n",
    "# y(t) \n",
    "ax = axs[1]\n",
    "colors = [clr['Dark blue'], clr['Sea blue'], clr['Land green'], clr['Cyan']]\n",
    "for i in range (4):\n",
    "    ax.plot(T_TRAIN[i], YT_TRAIN[i], marker = '.', markersize = 15, label = r'$(x_1, x_2) = ({:.2f}, {:.2f})$'.format(X_TRAIN[i][0], X_TRAIN[i][1]), color = colors[i]) \n",
    "for i in range (4, N_TRAIN):\n",
    "    ax.plot(T_TRAIN[i], YT_TRAIN[i], marker = '.', markersize = 15, color = clr['Sky blue'], alpha = 0.5) \n",
    "\n",
    "ax.set_xlabel('t', fontsize = 16)\n",
    "ax.set_ylabel('y', fontsize = 16)\n",
    "ax.set_title(r'$y(x_1, x_2, t)$', fontsize = 16)\n",
    "ax.legend()\n",
    "\n",
    "# x1, x2 - space\n",
    "ax = axs[0]\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel(r'$x_1$', fontsize = 16)\n",
    "ax.set_ylabel(r'$x_2$', fontsize = 16)\n",
    "ax.set_title(r'$(x_1, x_2)$', fontsize = 16)\n",
    "\n",
    "for i in range(4):\n",
    "    ax.scatter(X_TRAIN[i,0], X_TRAIN[i,1], marker = 'x', s = 60, color = clr['Dark blue'])\n",
    "ax.scatter(X_TRAIN[:,0], X_TRAIN[:,1], marker = 'x', s = 60, color = clr['Dark blue'])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOO example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Fit a GP to some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "\n",
    "X_Train = torch.tensor(T_TRAIN[2][::2].reshape(-1, 1), dtype=torch.float)\n",
    "Y_Train = torch.tensor(YT_TRAIN[2][::2], dtype=torch.float) \n",
    "\n",
    "# fig, ax = plt.subplots(figsize = (16, 8))\n",
    "\n",
    "# ax.plot(X_Train, Y_Train, marker = '.', markersize = 15, color = clr['Dark blue']) \n",
    "# ax.set_xlabel('t', fontsize = 16)\n",
    "# ax.set_ylabel('y', fontsize = 16)\n",
    "# ax.set_title(r'Training data', fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GP: Create and fit GP model to training data\n",
    "\n",
    "# Kernel\n",
    "ker: gpytorch.kernels.Kernel = gputils.gpytorch_kernel_Matern(\n",
    "    outputscale=KER_SCALE_TRUE, \n",
    "    lengthscale=KER_LENGTHSCALE_TRUE[-1].reshape(1)\n",
    "    )\n",
    "# Covariance matrix\n",
    "K: gpytorch.lazy.LazyEvaluatedKernelTensor = ker(X_Train)  # type: ignore\n",
    "# Create and fit GP model to training data\n",
    "model: ExactGPModel = ExactGPModel(\n",
    "                X_Train, Y_Train,                                                                   # Training data\n",
    "                gputils.gpytorch_mean_constant(0.0, fixed = True),                                  # Mean function\n",
    "                ker,                                                                                # Kernel\n",
    "                gputils.gpytorch_likelihood_gaussian(variance = 1e-6, fixed = False),               # Likelihood\n",
    "                '', '') # Name and path for save/load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GP: Draw 20 samples from GP model\n",
    "\n",
    "# 20 samples = 20 sampled time series of 100 data points each\n",
    "N_SAMPLES: int = 20  # number of samples to be drawn\n",
    "N_SAMPLE_POINTS: int = 100  # number of data points in each sample\n",
    "\n",
    "# Input: time (single dimension input data)\n",
    "t: torch.Tensor = torch.linspace(0, 1, N_SAMPLE_POINTS).reshape(-1, 1)  # time; shape=(N_SAMPLE_POINTS, 1)\n",
    "\n",
    "# Mean and covariance matrix\n",
    "model.eval_mode()\n",
    "m: torch.Tensor  # mean vector; shape=(N_SAMPLE_POINTS)\n",
    "v: torch.Tensor  # covariance matrix; shape=(N_SAMPLE_POINTS, N_SAMPLE_POINTS)\n",
    "m, v = model.predict(t, latent=True, full_cov=True, CG_tol = 0.001)\n",
    "\n",
    "# Add a jitter to covariance matrix\n",
    "w: Union[NDArray[Shape['N_SAMPLE_POINTS'], Float], NDArray[Shape['N_SAMPLE_POINTS'], Complex]]  # Eigenvalues of covariance matrix\n",
    "w, _ = np.linalg.eig(v)\n",
    "min_eigval: Union[float, complex] = w.min()\n",
    "print(min_eigval)\n",
    "jitter: float = abs(min_eigval*2)\n",
    "w, _ = np.linalg.eig(v + torch.eye(t.shape[0])*jitter)\n",
    "print(w.min())\n",
    "v_with_jitter: torch.Tensor = v + torch.eye(t.shape[0])*jitter\n",
    "\n",
    "# Distribution (incl. jitter)\n",
    "normal_rv: gpytorch.distributions.Distribution = gpytorch.distributions.MultivariateNormal(\n",
    "    mean=m,  # mean vector; shape=(N_SAMPLE_POINTS)\n",
    "    covariance_matrix=v_with_jitter,  # covariance matrix with jitter; shape=(N_SAMPLE_POINTS, N_SAMPLE_POINTS)\n",
    ")\n",
    "\n",
    "# Draw 20 samples from the distribution\n",
    "samples: NDArray[Shape['N_SAMPLES, N_SAMPLE_POINTS'], Float]\n",
    "samples = np.array([normal_rv.sample().numpy() for _ in range(N_SAMPLES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (16, 8))\n",
    "\n",
    "ax.scatter(X_Train, Y_Train, marker = '.', s = 150, color = clr['Dark blue']) \n",
    "ax.plot(t.flatten(), m, color = clr['Dark blue'])\n",
    "ax.fill_between(t.flatten(), m - 2*v.diagonal()**0.5, m + 2*v.diagonal()**0.5, color = clr['Cyan'], alpha = 0.1)\n",
    "ax.set_xlabel('t', fontsize = 16)\n",
    "ax.set_ylabel('y', fontsize = 16)\n",
    "ax.set_title(r'GP fitted to {} (noiseless) observations $y(t_1), y(t_2), ...$'.format(Y_Train.shape[0]), fontsize = 16)\n",
    "\n",
    "ax.plot(t, samples.T, color = 'red', alpha = 0.3, linewidth = 0.3)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Compute LOO errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Leave-One-Out (LOO) residuals for GP regression \n",
    "# (residual = observed - predicted)\n",
    "\n",
    "N_Y_TRAIN: int = Y_Train.shape[0]  # number of observations\n",
    "\n",
    "mean: Union[NDArray[Shape['N_Y_TRAIN'], Float], None]  # Mean of CV residuals\n",
    "cov: Union[NDArray[Shape['N_Y_TRAIN, N_Y_TRAIN'], Float], None]  # Covariance of CV residuals\n",
    "residuals_transformed: Union[NDArray[Shape['N_Y_TRAIN'], Float], None]  # The residuals transformed to the standard normal space\n",
    "\n",
    "# To compute the LOO residuals we only need the covariance matrix \n",
    "# and the training observations (converted to numpy arrays)\n",
    "mean, cov, residuals_transformed = gp_cv.loo(\n",
    "    K.numpy(),  # covariance matrix\n",
    "    Y_Train.numpy(),  # training observations\n",
    "    noise_variance = 0.,  # Gaussian noise\n",
    "    )\n",
    "\n",
    "# Assert that results are not None\n",
    "# Note:  This is necessary as loo() returns (None, None, None) in case \n",
    "#        the lower triangular cholesky factor could not successfully be computed.\n",
    "# @TODO: We should consider to change this behaviour in loo() \n",
    "#        and i.e. rather raise an exception instead. This would create a cleaner API.\n",
    "#        CLAROS, 2022-11-02\n",
    "assert mean is not None\n",
    "assert cov is not None\n",
    "assert residuals_transformed is not None\n",
    "\n",
    "# 'residuals_transformed' is the residuals transformed to the standard normal space.\n",
    "# We will see that this is not the same as normalizing the individual residuals (which will remain correlated)\n",
    "residuals_scaled: NDArray[Shape['N_Y_TRAIN'], Float] = mean / cov.diagonal()**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting residuals \n",
    "def plotres(ax, res, lbl = ''):\n",
    "    \"\"\"\n",
    "    Compute QQ plot of residuals 'res' and plot to ax\n",
    "    \"\"\"\n",
    "    q_sample, q_snorm, q_snorm_upper, q_snorm_lower = snorm_qq(res)\n",
    "    ax.scatter(q_snorm, q_sample, marker = 'o', facecolors='none', edgecolors='k')\n",
    "    ax.plot(q_snorm_upper, q_sample, 'k--')\n",
    "    ax.plot(q_snorm_lower, q_sample, 'k--')\n",
    "    ax.set_xlabel('Theoretical quantiles')\n",
    "    ax.set_ylabel('Sample quantiles')\n",
    "    ax.set_title('Normal Q-Q Plot of {}'.format(lbl))\n",
    "    line_min = min(q_snorm.min(), q_sample.min())*1.1\n",
    "    line_max = max(q_snorm.max(), q_sample.max())*1.1\n",
    "    ax.plot([line_min, line_max], [line_min, line_max], 'k')\n",
    "    ax.set_xlim(line_min, line_max)\n",
    "    ax.set_ylim(line_min, line_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals \n",
    "fig, axs = plt.subplots(ncols = 2, figsize = (18, 8))\n",
    "axs = axs.ravel()\n",
    "\n",
    "plotres(axs[0], residuals_transformed, 'Transformed LOO Residuals')\n",
    "plotres(axs[1], residuals_scaled, 'Standardized LOO Residuals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics: Union[Dict[str, Any], None]\n",
    "metrics = gp_metrics.evaluate_GP(K.numpy(), Y_Train.numpy(), noise_variance = 0) \n",
    "\n",
    "assert metrics is not None\n",
    "\n",
    "for key in ['log_marginal_likelihood', 'log_pseudo_likelihood', 'MSE']:\n",
    "    print(key, metrics[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multifold CV example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols = 2, figsize = (20, 8), gridspec_kw={'width_ratios': [1, 2]})\n",
    "axs = axs.ravel() \n",
    "\n",
    "# y(t) \n",
    "ax = axs[1]\n",
    "colors = [clr['Dark blue'], clr['Sea blue'], clr['Land green'], clr['Cyan']]\n",
    "i = 2\n",
    "ax.plot(T_TRAIN[i], YT_TRAIN[i], marker = '.', markersize = 15, label = r'$(x_1, x_2) = ({:.2f}, {:.2f})$'.format(X_TRAIN[i][0], X_TRAIN[i][1]), color = clr['Land green']) \n",
    "for i in [0, 1]:\n",
    "    ax.plot(T_TRAIN[i], YT_TRAIN[i], marker = '.', markersize = 15, label = r'$(x_1, x_2) = ({:.2f}, {:.2f})$'.format(X_TRAIN[i][0], X_TRAIN[i][1]), color = clr['Dark blue']) \n",
    "for i in range (3, N_TRAIN):\n",
    "    ax.plot(T_TRAIN[i], YT_TRAIN[i], marker = '.', markersize = 15, color = clr['Sky blue'], alpha = 0.5) \n",
    "\n",
    "ax.set_xlabel('t', fontsize = 16)\n",
    "ax.set_ylabel('y', fontsize = 16)\n",
    "ax.set_title(r'$y(x_1, x_2, t)$', fontsize = 16)\n",
    "ax.legend()\n",
    "\n",
    "# x1, x2 - space\n",
    "ax = axs[0]\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel(r'$x_1$', fontsize = 16)\n",
    "ax.set_ylabel(r'$x_2$', fontsize = 16)\n",
    "ax.set_title(r'$(x_1, x_2)$', fontsize = 16)\n",
    "\n",
    "i = 2\n",
    "ax.scatter(X_TRAIN[i,0], X_TRAIN[i,1], marker = 'o', facecolors='none', edgecolors=clr['Land green'], s = 140)\n",
    "\n",
    "for i in [0, 1]:\n",
    "    ax.scatter(X_TRAIN[i,0], X_TRAIN[i,1], marker = 'o', facecolors='none', edgecolors=clr['Dark blue'], s = 140)\n",
    "\n",
    "ax.scatter(X_TRAIN[:,0], X_TRAIN[:,1], marker = 'x', s = 60, color = clr['Cyan'])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Compute GP covariance matrix from training inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_Train = torch.tensor(XT_TRAIN, dtype=torch.float)\n",
    "Y_Train = torch.tensor(torch.cat(YT_TRAIN), dtype=torch.float) \n",
    "\n",
    "# Kernel\n",
    "ker: gpytorch.kernels.Kernel = gputils.gpytorch_kernel_Matern(\n",
    "    outputscale=KER_SCALE_TRUE, \n",
    "    lengthscale=KER_LENGTHSCALE_TRUE,\n",
    "    )\n",
    "\n",
    "# Covariance matrix\n",
    "K: gpytorch.lazy.LazyEvaluatedKernelTensor = ker(X_Train)  # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Define folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, the folds are naturally given from the training data, as a \"single observation\" corresponds to \n",
    "# observing the function y(t), at some times t_1, t_2, ..., for one specific value of (x_1, x_2)\n",
    "folds_startstop = np.array([0] + [Y.shape[0] for Y in YT_TRAIN]).cumsum()\n",
    "folds = [list(range(folds_startstop[i], folds_startstop[i+1])) for i in range(len(folds_startstop)-1)]\n",
    "\n",
    "print('There are a total of {} folds with the following indices:'.format(len(folds)))\n",
    "display(folds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Compute CV residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute the LOO residuals we only need the covariance matrix and the training observations (converted to numpy arrays)\n",
    "mean, cov, residuals_transformed = gp_cv.multifold(K.numpy(), Y_Train.numpy(), folds, noise_variance = 0) \n",
    "\n",
    "# 'residuals_transformed' is the residuals transformed to the standard normal space\n",
    "# we will see that this is not the same normalizing the individual residuals (which will remain correlated)\n",
    "residuals_scaled = mean / cov.diagonal()**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals \n",
    "fig, axs = plt.subplots(ncols = 2, figsize = (18, 8))\n",
    "axs = axs.ravel()\n",
    "\n",
    "plotres(axs[0], residuals_transformed, 'Transformed LOO Residuals')\n",
    "plotres(axs[1], residuals_scaled, 'Standardized LOO Residuals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color by each fold \n",
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "\n",
    "q_sample, q_snorm, q_snorm_upper, q_snorm_lower = snorm_qq(residuals_transformed)\n",
    "ax.plot(q_snorm_upper, q_sample, 'k--')\n",
    "ax.plot(q_snorm_lower, q_sample, 'k--')\n",
    "ax.set_xlabel('Theoretical quantiles', fontsize = 15)\n",
    "ax.set_ylabel('Sample quantiles', fontsize = 15)\n",
    "ax.set_title('Normal Q-Q Plot of Transformed LOO Residuals', fontsize = 20)\n",
    "line_min = min(q_snorm.min(), q_sample.min())*1.1\n",
    "line_max = max(q_snorm.max(), q_sample.max())*1.1\n",
    "ax.plot([line_min, line_max], [line_min, line_max], 'k')\n",
    "ax.set_xlim(line_min, line_max)\n",
    "ax.set_ylim(line_min, line_max)\n",
    "\n",
    "for i in range(len(folds)):\n",
    "    ax.scatter(q_snorm[folds[i]], q_sample[folds[i]], marker = 'o', label = 'Fold_{}'.format(i))\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "\n",
    "y_obs = Y_Train\n",
    "y_pred_mean = y_obs - mean\n",
    "y_pred_std = cov.diagonal()**0.5\n",
    "\n",
    "ax.set_xlabel('Observed', fontsize = 15)\n",
    "ax.set_ylabel('Predicted', fontsize = 15)\n",
    "\n",
    "line_min = min(y_obs.min(), y_pred_mean.min())*1.1\n",
    "line_max = max(y_obs.max(), y_pred_mean.max())*1.1\n",
    "ax.plot([line_min, line_max], [line_min, line_max], 'k')\n",
    "ax.set_xlim(line_min, line_max)\n",
    "ax.set_ylim(line_min, line_max)\n",
    "\n",
    "ax.errorbar(y_obs, y_pred_mean, yerr=2*y_pred_std, fmt='o', color = 'k')\n",
    "\n",
    "for i in range(len(folds)):\n",
    "    ax.errorbar(y_obs[folds[i]], y_pred_mean[folds[i]], yerr=2*y_pred_std[folds[i]], fmt='o', label = 'Fold_{}'.format(i))\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "\n",
    "y_obs = Y_Train\n",
    "y_pred_mean = y_obs - mean\n",
    "y_pred_std = cov.diagonal()**0.5\n",
    "\n",
    "ax.set_xlabel('Observed', fontsize = 15)\n",
    "ax.set_ylabel('Predicted', fontsize = 15)\n",
    "\n",
    "line_min = min(y_obs.min(), y_pred_mean.min())*1.1\n",
    "line_max = max(y_obs.max(), y_pred_mean.max())*1.1\n",
    "ax.plot([line_min, line_max], [line_min, line_max], 'k')\n",
    "ax.set_xlim(line_min, line_max)\n",
    "ax.set_ylim(line_min, line_max)\n",
    "\n",
    "ax.errorbar(y_obs, y_pred_mean, yerr=2*y_pred_std, fmt='o', color = 'k')\n",
    "\n",
    "for i in range(len(folds)):\n",
    "    ax.errorbar(y_obs[folds[i]], y_pred_mean[folds[i]], yerr=2*y_pred_std[folds[i]], fmt='o', label = 'Fold_{}'.format(i))\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the 'metrics.evaluate_GP()' function to compute a set of relevant evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = gp_metrics.evaluate_GP(K.numpy(), Y_Train.numpy(), folds, noise_variance = 0) \n",
    "\n",
    "for key in ['log_marginal_likelihood', 'log_pseudo_likelihood', 'MSE']:\n",
    "    print(key, metrics[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gp_metrics.evaluate_GP(K.numpy(), Y_Train.numpy(), folds, noise_variance = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOO for the same data (not correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = gp_metrics.evaluate_GP(K.numpy(), Y_Train.numpy(), folds = None, noise_variance = 0) \n",
    "\n",
    "for key in ['log_marginal_likelihood', 'log_pseudo_likelihood', 'MSE']:\n",
    "    print(key, metrics[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute the LOO residuals we only need the covariance matrix and the training observations (converted to numpy arrays)\n",
    "mean, cov, residuals_transformed = gp_cv.loo(K.numpy(), Y_Train.numpy(), noise_variance = 0) \n",
    "\n",
    "# 'residuals_transformed' is the residuals transformed to the standard normal space\n",
    "# we will see that this is not the same normalizing the individual residuals (which will remain correlated)\n",
    "residuals_scaled = mean / cov.diagonal()**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color by each fold \n",
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "\n",
    "q_sample, q_snorm, q_snorm_upper, q_snorm_lower = snorm_qq(residuals_transformed)\n",
    "ax.plot(q_snorm_upper, q_sample, 'k--')\n",
    "ax.plot(q_snorm_lower, q_sample, 'k--')\n",
    "ax.set_xlabel('Theoretical quantiles', fontsize = 15)\n",
    "ax.set_ylabel('Sample quantiles', fontsize = 15)\n",
    "ax.set_title('Normal Q-Q Plot of Transformed LOO Residuals', fontsize = 20)\n",
    "line_min = min(q_snorm.min(), q_sample.min())*1.1\n",
    "line_max = max(q_snorm.max(), q_sample.max())*1.1\n",
    "ax.plot([line_min, line_max], [line_min, line_max], 'k')\n",
    "ax.set_xlim(line_min, line_max)\n",
    "ax.set_ylim(line_min, line_max)\n",
    "\n",
    "for i in range(len(folds)):\n",
    "    ax.scatter(q_snorm[folds[i]], q_sample[folds[i]], marker = 'o', label = 'Fold_{}'.format(i))\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_multi, cov_multi, residuals_transformed_multi = gp_cv.multifold(K.numpy(), Y_Train.numpy(), folds, noise_variance = 0) \n",
    "residuals_scaled_multi = mean_multi / cov_multi.diagonal()**0.5\n",
    "mean_loo, cov_loo, residuals_transformed_loo = gp_cv.loo(K.numpy(), Y_Train.numpy(), noise_variance = 0) \n",
    "residuals_scaled_loo = mean_loo / cov_loo.diagonal()**0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_transformed_loo - residuals_transformed_multi\n",
    "mean_multi - mean_loo\n",
    "cov_multi - cov_loo\n",
    "residuals_scaled_multi - residuals_scaled_loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals \n",
    "fig, axs = plt.subplots(ncols = 2, figsize = (18, 8))\n",
    "axs = axs.ravel()\n",
    "\n",
    "plotres(axs[0], residuals_scaled_multi, 'residuals_scaled_multi')\n",
    "plotres(axs[1], residuals_scaled_loo, 'residuals_scaled_loo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37a94fe6c21872b644e894c742197238ed8676af7dff7c9a4646e8671405b66d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
